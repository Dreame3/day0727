```
new LoginEvent("1","192.168.0.1","fail"),
new LoginEvent("1","192.168.0.2","fail"),
new LoginEvent("1","192.168.0.3","fail"),
new LoginEvent("2","192.168.10.10","success")
```

userId: 1; keyedStream

```
new LoginEvent("1","192.168.0.1","fail"),
new LoginEvent("1","192.168.0.2","fail"),
new LoginEvent("1","192.168.0.3","fail"),
```

userId: 2; keyedStream

```\
new LoginEvent("2","192.168.10.10","success")
```

pattern: 严格紧邻

```
pattern = (begin:loginfail, next:loginfail)
```

没有时间戳，所以默认使用processing time

```
new LoginEvent("1","192.168.0.1","fail"),
new LoginEvent("3","192.168.0.2","fail"),
```

匹配出来的两组紧邻事件

```
new LoginEvent("1","192.168.0.1","fail"),
new LoginEvent("1","192.168.0.2","fail"),
```

```
new LoginEvent("1","192.168.0.2","fail"),
new LoginEvent("1","192.168.0.3","fail"),
```



Flink-CEP:

1，登陆事件检测

LoginEvent Stream —> key by ——>userid: LoginEvent Stream ——>pattern matching —> select(定义报警输出) ——> LoginWarning Stream ——> print()

2，温度事件检测

TemperatureEvent Stream —> pattern matching(filter > 27.0) ——> select(定义报警输出) ——> Alert Stream ——> print()



3，HotItems:

滑动窗口热门商品

滑动窗口: [9:00, 10:00), [9:05, 10:05), ...

滑动窗口，Event Time，Watermark

Flink即使是处理离线日志文件，思路是把离线文件当成流

log.file

```
log1
log2
log3
```

(log1, log2, log3)

流批统一的思路

1. 创建流执行环境
2. 设置流的执行环境所用的时间是：事件时间(event time)，如果不设置，默认使用processing time
3. csv文件转化为UserBehavior POJOs事件流，createInput(csvInput, pojoType)，transformer操作
4. UserBehavior1, UserBehavior2, UserBehavior3,...
5. 为流设置水位线watermark，由于已经知道时间戳是递增的，所以直接将时间戳作为水位线插入到流中
6. `DataStream`: UserBehavior1, UserBehavior2, UserBehavior3,… ——>`DataStream` : UserBehavior1, Watermark(UserBehavior1.timestamp), UserBehavior2, Watermark(UserBehavior2.timestamp), UserBehavior3, Watermark(UserBehavior3.timestamp), ——> filter(过滤出浏览事件pv) ——> `DataStream`
7. `UserBehavior DataStream with watermark and with pv` ——>keyby(商品id) ——>
   1. itemid==1: `DataStream`: UserBahavior1(itemid==1), UserBahavior2(itemid==1),UserBahavior3(itemid==1),UserBahavior4(itemid==1)
   2. itemid==2: `DataStream`: UserBahavior1(itemid==2), UserBahavior2(itemid==2),UserBahavior3(itemid==2),UserBahavior4(itemid==2)
8. 对流设置滑动窗口(Time.minutes(60), Time.minutes(5)), 窗口大小1个小时，每次滑动距离是5分钟
9. 做聚合
   1. 定义聚合操作，比如加法，乘法，字符串拼接等等, CountAgg
   2. 聚合完成以后输出的数据流的类型. `UserBehavior DataStream` —> ?, WindowFunction
      1. 例如itemid==1: `DataStream`: UserBahavior1(itemid==1), `UserBahavior2(itemid==1),UserBahavior3(itemid==1),UserBahavior4(itemid==1)`这四个事件在[9:00, 10:00); `DataStream`: `UserBahavior2(itemid==1), UserBahavior3(itemid==1),UserBahavior4(itemid==1)`,UserBahavior5(itemid==1)这四个事件在[9:05, 10:05)
      2. UserBehavior Stream` —> ItemViewCount(itemid: 1, windowend: 10:00, count: 4), ItemViewCount(itemid: 1, windowend: 10:05, count: 4)——>
         1. windowend==10:00: ItemViewCount(itemid: 1, windowend: 10:00, count: 4), ItemViewCount(itemid: 11, windowend: 10:00, count: 30)
         2. Windowend==10:05: ItemViewCount(itemid: 1, windowend: 10:05, count: 4), ItemViewCount(itemid: 11, windowend: 10:05, count: 40)
         3. ——> 触发processFunction，做排序
10. processFunction的生命周期中，新建一个state，把数据做checkpoint持久化
11. DataStream ItemViewCount ——> 将事件保存到state 中: itemState.add(input); 例如ItemViewCount(itemid: 1, windowend: 10:00, count: 4)保存到state中 —> 设置定时(10:00 + 1ms)事件(时间到了10:00+1ms的水位线时以后的回调函数）—> onTimer回调函数，把state中的数据放到allItems里面去，然后清空state ——> 排序 —> print()













埋点：

1. cnzz，友盟，talkingdata
2. elasticsearch(3台) + logstash(2台) + kibana(1台)
3. spark
4. flink





map

filter: grep

Reduce: groupby, reduceby, keyby, foldL, foldR, aggregate

Flatten: 拍扁

Flatmap = flatten + map

