== 第四章 Flink DataStream API

[graphviz, dot-example, svg]
----
digraph g {
    rankdir=LR
    environment -> source
    source -> transform
    transform -> sink
}
----

=== 4.1 Environment

*getExecutionEnvironment*

创建一个执行环境，表示当前执行程序的上下文。如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。

[source,scala]
----
// 获取执行环境
val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
// 设置流的时间为Event Time
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
// 设置并行度为1，如果不设置，那么默认为当前机器的cpu的数量
env.setParallelism(1)
----

=== 4.2 Source

以Kafka消息队列的数据为数据来源

[source,scala]
----
object ScalaHotItems {

  def main(args: Array[String]): Unit = {
    val properties = new Properties()
    properties.setProperty("bootstrap.servers", "localhost:9092")
    properties.setProperty("group.id", "consumer-group")
    properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.setProperty("auto.offset.reset", "latest")
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(1)
    val stream = env
      // source为来自Kafka的数据，这里我们实例化一个消费者，topic为hotitems
      .addSource(new FlinkKafkaConsumer[String]("hotitems", new SimpleStringSchema(), properties))
      .print()

    // 执行DAG
    env.execute("Hot Items Job")
  }
}
----

https://flink.apache.org/features/2018/03/01/end-to-end-exactly-once-apache-flink.html

文档

=== 4.3 Basic Transformations

基本转换算子

==== 4.3.1 map

image::map.png[]

[source,scala]
----
val streamMap = stream.map { x => x * 2 }
----

==== 4.3.2 flatMap

image::flatmap.png[]

白框不变，黑框复制，灰框过滤

flatmap类似map，但可以生成0个或者1个或者多个数据

flatmap可以实现map和filter

[source,scala]
----
val streamFlatMap = stream.flatMap{
  x => x.split(" ")
}
----

==== 4.3.3 Filter

image::filter.png[]

[source,scala]
----
val streamFilter = stream.filter{
  x => x == 1
}
----

=== 4.4 KeyedStream Transformations

==== 4.4.1 keyBy

分流算子 DataStream -> KeyedStream

image::keyby.png[]

黑色去一条流，剩下的去另一条流

[source,scala]
----
// 以数据的id为key分流
stream.keyBy(r => r.id)
// 以case class的word字段为key分流
stream.keyBy("word")
// 以Tuple的第0个元素为key分流
stream.keyBy(0)
----

==== 4.4.2 Rolling Aggregations

针对KeyedStream的每一条流做聚合

* sum()
* min()
* max()
* minBy()
* maxBy()

[source,scala]
----
val inputStream: DataStream[(Int, Int, Int)] = 
env.fromElements(
  (1, 2, 2), (2, 3, 1), (2, 2, 4), (1, 5, 3))

val resultStream: DataStream[(Int, Int, Int)] = inputStream
  .keyBy(0)
  .sum(1)
----

==== 4.4.3 Reduce

KeyedStream → DataStream：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。

[source,scala]
----
val inputStream: DataStream[(String, List[String])] =
env.fromElements(
  ("en", List("tea")), ("fr", List("vin")), ("en", List("cake"))
)

val resultStream: DataStream[(String, List[String])] =
inputStream
  .keyBy(0)
  .reduce((x, y) => x._1, x._2 ::: y._2)
----

=== 4.5 Multistream Transformations

==== 4.5.1 Union

==== 4.5.2 Connect, Comap and Coflatmap

==== 4.5.3 Split and Select

=== 4.6 Distribution Transformations

* Random
* Round-Robin
* Rescale
* Broadcast
* Global
* Custom

==== 4.3.6 Split 和 Select

Split

DataStream → SplitStream：根据某些特征把一个DataStream拆分成两个或者多个DataStream。
Select

SplitStream→DataStream：从一个SplitStream中获取一个或者多个DataStream。

需求：把appstore和其他的渠道的数据单独拆分出来，做成两个流

[source,scala]
----
     // 将appstore与其他渠道拆分拆分出来  成为两个独立的流
val splitStream: SplitStream[StartUpLog] = startUplogDstream.split { startUplog =>
  var flags:List[String] =  null
  if ("appstore" == startUplog.ch) {
    flags = List(startUplog.ch)
  } else {
    flags = List("other" )
  }
  flags
}
val appStoreStream: DataStream[StartUpLog] = splitStream.select("appstore")
appStoreStream.print("apple:").setParallelism(1)
val otherStream: DataStream[StartUpLog] = splitStream.select("other")
otherStream.print("other:").setParallelism(1)
----

==== 4.3.7 Connect和CoMap

DataStream,DataStream → ConnectedStreams：连接两个保持他们类型的数据流，两个数据流被Connect之后，只是被放在了一个同一个流中，内部依然保持各自的数据和形式不发生任何变化，两个流相互独立。
CoMap,CoFlatMap

ConnectedStreams → DataStream：作用于ConnectedStreams上，功能与map和flatMap一样，对ConnectedStreams中的每一个Stream分别进行map和flatMap处理。

[source,scala]
----
//合并以后打印
val connStream: ConnectedStreams[StartUpLog, StartUpLog] = appStoreStream.connect(otherStream)
val allStream: DataStream[String] = connStream.map(
  (log1: StartUpLog) => log1.ch,
  (log2: StartUpLog) => log2.ch
)
allStream.print("connect::")
----

==== 4.3.8 Union

DataStream → DataStream：对两个或者两个以上的DataStream进行union操作，产生一个包含所有DataStream元素的新DataStream。注意:如果你将一个DataStream跟它自己做union操作，在新的DataStream中，你将看到每一个元素都出现两次。

[source,scala]
----
//合并以后打印
val unionStream: DataStream[StartUpLog] = appStoreStream.union(otherStream)
unionStream.print("union:::")
----

Connect与 Union 区别：
1 、 Union之前两个流的类型必须是一样，Connect可以不一样，在之后的coMap中再去调整成为一样的。
2 Connect只能操作两个流，Union可以操作多个

=== 4.4 Sink
   Flink没有类似于spark中foreach方法，让用户进行迭代的操作。虽有对外的输出操作都要利用Sink完成。最后通过类似如下方式完成整个任务最终输出操作。

   myDstream.addSink(new MySink(xxxx)) 

 官方提供了一部分的框架的sink。除此以外，需要用户自定义实现sink。   

----
<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-connector-kafka-0.11 -->
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-kafka-0.11_2.11</artifactId>
    <version>1.7.0</version>
</dependency>
----

mykafkaUtil中增加方法

----
def getProducer(topic:String): FlinkKafkaProducer011[String] ={
  new FlinkKafkaProducer011[String](brokerList,topic,new SimpleStringSchema())
}
----

主函数中添加sink

----
val myKafkaProducer: FlinkKafkaProducer011[String] = MyKafkaUtil.getProducer("channel_sum")
 
sumDstream.map( chCount=>chCount._1+":"+chCount._2 ).addSink(myKafkaProducer)
----

==== 4.4.1 Elasticsearch

----
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch6_2.11</artifactId>
    <version>1.7.0</version>
</dependency>

<dependency>
    <groupId>org.apache.httpcomponents</groupId>
    <artifactId>httpclient</artifactId>
    <version>4.5.3</version>
</dependency>
----

添加MyEsUtil

----
import java.util

import com.alibaba.fastjson.{JSON, JSONObject}
import org.apache.flink.api.common.functions.RuntimeContext
import org.apache.flink.streaming.connectors.elasticsearch.{ElasticsearchSinkFunction, RequestIndexer}
import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink
import org.apache.http.HttpHost
import org.elasticsearch.action.index.IndexRequest
import org.elasticsearch.client.Requests

object MyEsUtil {
  
  val httpHosts = new util.ArrayList[HttpHost]
  httpHosts.add(new HttpHost("hadoop1",9200,"http"))
   httpHosts.add(new HttpHost("hadoop2",9200,"http"))
   httpHosts.add(new HttpHost("hadoop3",9200,"http"))

  def  getElasticSearchSink(indexName:String):  ElasticsearchSink[String]  ={
    val esFunc = new ElasticsearchSinkFunction[String] {
      override def process(element: String, ctx: RuntimeContext, indexer: RequestIndexer): Unit = {
        println("试图保存："+element)
        val jsonObj: JSONObject = JSON.parseObject(element)
        val indexRequest: IndexRequest = Requests.indexRequest().index(indexName).`type`("_doc").source(jsonObj)
        indexer.add(indexRequest)
        println("保存1条")
      }
    }

    val sinkBuilder = new ElasticsearchSink.Builder[String](httpHosts, esFunc)

    //刷新前缓冲的最大动作量
    sinkBuilder.setBulkFlushMaxActions(10)

    sinkBuilder.build()
  }
}
----

在main方法中调用

[source,scala]
----
// 明细发送到es 中
val esSink: ElasticsearchSink[String] = MyEsUtil.getElasticSearchSink("gmall0503_startup")
dstream.addSink(esSink)
----