== 第四章 Flink DataStream API

[graphviz, dot-example, svg]
----
digraph g {
    rankdir=LR
    environment -> source
    source -> transform
    transform -> sink
}
----

=== 4.1 Environment

*getExecutionEnvironment*

创建一个执行环境，表示当前执行程序的上下文。如果程序是独立调用的，则此方法返回本地执行环境；如果从命令行客户端调用程序以提交到集群，则此方法返回此集群的执行环境，也就是说，getExecutionEnvironment会根据查询运行的方式决定返回什么样的运行环境，是最常用的一种创建执行环境的方式。

[source,scala]
----
// 获取执行环境
val env: ExecutionEnvironment = ExecutionEnvironment.getExecutionEnvironment
// 设置流的时间为Event Time
env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
// 设置并行度为1，如果不设置，那么默认为当前机器的cpu的数量
env.setParallelism(1)
----

=== 4.2 Source

以Kafka消息队列的数据为数据来源

[source,scala]
----
object ScalaHotItems {

  def main(args: Array[String]): Unit = {
    val properties = new Properties()
    properties.setProperty("bootstrap.servers", "localhost:9092")
    properties.setProperty("group.id", "consumer-group")
    properties.setProperty("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer")
    properties.setProperty("auto.offset.reset", "latest")
    val env = StreamExecutionEnvironment.getExecutionEnvironment
    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
    env.setParallelism(1)
    val stream = env
      // source为来自Kafka的数据，这里我们实例化一个消费者，topic为hotitems
      .addSource(new FlinkKafkaConsumer[String]("hotitems", new SimpleStringSchema(), properties))
      .print()

    // 执行DAG
    env.execute("Hot Items Job")
  }
}
----

https://flink.apache.org/features/2018/03/01/end-to-end-exactly-once-apache-flink.html

文档

我们也可以自定义Source，这里举一个例子。

[source,scala]
----
import java.util.Calendar

import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction
import org.apache.flink.streaming.api.functions.source.SourceFunction.SourceContext

import scala.util.Random

/**
  * Flink SourceFunction to generate SensorReadings with random temperature values.
  *
  * Each parallel instance of the source simulates 10 sensors which emit one sensor
  * reading every 100 ms.
  *
  * Note: This is a simple data-generating source function that does not checkpoint its state.
  * In case of a failure, the source does not replay any data.
  */

// 传感器id，时间戳，温度
case class SensorReading(id: String, timestamp: Long, temperature: Double)

// 需要extends RichParallelSourceFunction, 泛型为SensorReading
class SensorSource extends RichParallelSourceFunction[SensorReading] {

  // flag indicating whether source is still running.
  // flag: 表示数据源是否还在正常运行
  var running: Boolean = true

  /** run() continuously emits SensorReadings by emitting them through the SourceContext. */
  // run()函数连续的发送SensorReading数据，使用SourceContext
  // 需要override
  override def run(srcCtx: SourceContext[SensorReading]): Unit = {

    // initialize random number generator
    // 初始化随机数发生器
    val rand = new Random()
    // look up index of this parallel task
    // 查找当前运行时上下文的任务的索引
    val taskIdx = this.getRuntimeContext.getIndexOfThisSubtask

    // initialize sensor ids and temperatures
    // 初始化10个(温度传感器的id, 温度值)元组
    var curFTemp = (1 to 10).map {
      // nextGaussian产生高斯随机数
      i => ("sensor_" + (taskIdx * 10 + i), 65 + (rand.nextGaussian() * 20))
    }

    // emit data until being canceled
    // 无限循环，产生数据流
    while (running) {

      // update temperature
      // 更新温度
      curFTemp = curFTemp.map( t => (t._1, t._2 + (rand.nextGaussian() * 0.5)) )
      // get current time
      // 获取当前时间戳
      val curTime = Calendar.getInstance.getTimeInMillis

      // emit new SensorReading
      // 发射新的传感器数据, 注意这里srcCtx.collect
      curFTemp.foreach( t => srcCtx.collect(SensorReading(t._1, curTime, t._2)))

      // wait for 100 ms
      Thread.sleep(100)
    }

  }

  /** Cancels this SourceFunction. */
  // override cancel函数
  override def cancel(): Unit = {
    running = false
  }

}
----

使用方法

[source,scala]
----
// ingest sensor stream
val sensorData: DataStream[SensorReading] = env
  // SensorSource generates random temperature readings
  .addSource(new SensorSource)
----

=== 4.3 Basic Transformations

基本转换算子

==== 4.3.1 map

image::map.png[]

[source,scala]
----
val streamMap = stream.map { x => x * 2 }
----

==== 4.3.2 flatMap

image::flatmap.png[]

白框不变，黑框复制，灰框过滤

flatmap类似map，但可以生成0个或者1个或者多个数据

flatmap可以实现map和filter

[source,scala]
----
val streamFlatMap = stream.flatMap{
  x => x.split(" ")
}
----

==== 4.3.3 Filter

image::filter.png[]

[source,scala]
----
val streamFilter = stream.filter{
  x => x == 1
}
----

=== 4.4 KeyedStream Transformations

==== 4.4.1 keyBy

分流算子 DataStream -> KeyedStream

image::keyby.png[]

黑色去一条流，剩下的去另一条流

[source,scala]
----
// 以数据的id为key分流
stream.keyBy(r => r.id)
// 以case class的word字段为key分流
stream.keyBy("word")
// 以Tuple的第0个元素为key分流
stream.keyBy(0)
----

==== 4.4.2 Rolling Aggregations

针对KeyedStream的每一条流做聚合

* sum()
* min()
* max()
* minBy()
* maxBy()

[source,scala]
----
val inputStream: DataStream[(Int, Int, Int)] = 
env.fromElements(
  (1, 2, 2), (2, 3, 1), (2, 2, 4), (1, 5, 3))

val resultStream: DataStream[(Int, Int, Int)] = inputStream
  .keyBy(0)
  .sum(1)
----

==== 4.4.3 Reduce

KeyedStream -> DataStream：一个分组数据流的聚合操作，合并当前的元素和上次聚合的结果，产生一个新的值，返回的流中包含每一次聚合的结果，而不是只返回最后一次聚合的最终结果。

[source,scala]
----
val inputStream: DataStream[(String, List[String])] =
env.fromElements(
  ("en", List("tea")), ("fr", List("vin")), ("en", List("cake"))
)

val resultStream: DataStream[(String, List[String])] =
inputStream
  .keyBy(0)
  .reduce((x, y) => x._1, x._2 ::: y._2)
----

=== 4.5 Multistream Transformations

==== 4.5.1 Union

image::union.png[]

将事件类型相同的多条DataStream合并到一起，在进入到合流时，使用FIFO先进先出的原则。Union算子不会对事件的顺序做处理。

[source,scala]
----
val parisStream: DataStream[SensorReading] = ...
val tokyoStream: DataStream[SensorReading] = ...
val rioStream: DataStream[SensorReading] = ...
val allCities: DataStream[SensorRreading] = parisStream
  .union(tokyoStream, rioStream)
----

==== 4.5.2 Connect, Comap and Coflatmap

联合两条流的事件是非常常见的流处理需求。例如监控一片森林然后发出高危的火警警报。报警的Application接收两条流，一条是温度传感器传回来的数据，一条是烟雾传感器传回来的数据。当两条流都超过各自的阈值时，报警。

DataStream.connect()方法就实现了这个功能。DataStream -> ConnectedStreams。

[source,scala]
----
// first stream
val first: DataStream[Int] = ...
// second stream
val second: DataStream[String] = ...

// connect streams
val connected: ConnectedStreams[Int, String] =
first.connect(second)
----

ConnectedStream提供了map和flatMap方法。

* map: 需要CoMapFunction作为参数
* flatMap: 需要CoFlatMapFunction作为参数

CoMapFunction和CoFlatMapFunction都需要两条输入流的类型，还需要输出流的类型，还需要定义两个方法，一个方法对应一条流。map1()和flatMap1()处理第一条流，map2()和flatMap2()处理第二条流。

----
// IN1: 第一条流的事件类型
// IN2: 第二条流的事件类型
// OUT: 输出流的事件类型
CoMapFunction[IN1, IN2, OUT]
    > map1(IN1): OUT
    > map2(IN2): OUT

CoFlatMapFunction[IN1, IN2, OUT]
    > flatMap1(IN1, Collector[OUT]): Unit
    > flatMap2(IN2, Collector[OUT]): Unit
----

两条流直接connect，其实是没有意义的。因为我们相当于将两条流随机的合并成了一条流，结果没什么价值。为了获得确定性的结果，connect必须和keyBy或者broadcast一起使用。

*keyBy*

以两条流的事件的第一个元素为key，做连接join

[source,scala]
----
val one: DataStream[(Int, Long)] = ...
val two: DataStream[(Int, String)] = ...

// keyBy two connected streams
val keyedConnect1: ConnectedStreams[(Int, Long), (Int, String)] =
one
  .connect(two)
  .keyBy(0, 0) // key both input streams on first attribute

// alternative: connect two keyed streams
val keyedConnect2: ConnectedStreams[(Int, Long), (Int, String)] =
one.keyBy(0).connect(two.keyBy(0))
----

以上程序其实相当于实现了SQL中的JOIN语义。

==== 4.5.3 Split and Select

Split是Union的反函数。

image::split.png[]

DataStream -> SplitStream

[source,scala]
----
val inputStream: DataStream[(Int, String)] = ...

val splitted: SplitStream[(Int, String)] = inputStream
  .split(t => if (t._1 > 1000) Seq("large") else Seq("small"))

val large: DataStream[(Int, String)] = splitted.select("large")
val small: DataStream[(Int, String)] = splitted.select("small")
val all: DataStream[(Int, String)] = splitted.select("small", "large")
----

Connect与 Union 区别：
1 Union之前两个流的类型必须是一样，Connect可以不一样，在之后的CoMapFunction中再去调整成为一样的。
2 Connect只能操作两个流，Union可以操作多个。

=== 4.6 支持的数据类型

*Primitives*

[source,scala]
----
val numbers: DataStream[Long] = env.fromElements(1L, 2L, 3L, 4L)
numbers.map( n => n + 1 )
----

*Tuples*

[source,scala]
----
val persons: DataStream[(String, Integer)] =
env.fromElements(
  ("Adam", 17),
  ("Sarah", 23)
)

persons.filter(p => p._2 > 18)
----

*Scala case classes*

[source,scala]
----
case class Person(name: String, age: Int)

val persons: DataStream[Person] = env.fromElements(
  Person("Adam", 17),
  Person("Sarah", 23)
)

persons.filter(p => p.age > 18)
----

*others*

* Hadoop Writable types
* Java's POJOs, ArrayList, HashMap, Enum
* ...

=== 4.7 keyBy相关用法

[source,scala]
----
val input: DataStream[(Int, String, Long)] = ...
val keyed = input.keyBy(1)
val keyed2 = input.keyBy(1, 2)
----

[source,scala]
----
case class SensorReading(
  id: String,
  timestamp: Long,
  temperature: Double
)

val sensorStream: DataStream[SensorReading] = ...
val keyedSensors = sensorStream.keyBy("id")
----

[source,scala]
----
val input: DataStream[(Int, String, Long)] = ...
val keyed1 = input.keyBy("2") // key by 3rd field
val keyed2 = input.keyBy("_1") // key by 1st field
----

[source,scala]
----
case class Address (
  address: String,
  zip: String,
  country: String
)

case class Person (
  name: String,
  birthday: (Int, Int, Int), // year, month, day
  address: Address
)

val persons: DataStream[Person] = ...
persons.keyBy("address.zip") // key by nested POJO field
persons.keyBy("birthday._1") // key by field of nested tuple
persons.keyBy("birthday._") // key by all fields of nested tuple
----

[source,scala]
----
val sensorData: DataStream[SensorReading] = ...
val byId: KeyedStream[SensorReading, String] = sensorData.keyBy(r => r.id)
----

[source,scala]
----
val input: DataStream[(Int, Int)] = ...
val keyedStream = input.keyBy(value => math.max(value._1, value._2))
----

=== 4.8 实现UDF函数，更细粒度的控制流

==== 4.8.1 Function Classes

Flink暴露了所有udf函数的接口(实现方式为接口或者抽象类)。例如MapFunction, FilterFunction, ProcessFunction等等。

例子实现了FilterFunction接口

[source,scala]
----
class FilterFilter extends FilterFunction[String] {
  override def filter(value: String): Boolean = {
    value.contains("flink")
  }
}

val flinkTweets = tweets.filter(new FlinkFilter)
----

还可以将函数实现成匿名类

[source,scala]
----
val flinkTweets = tweets.filter(
  new RichFilterFunction[String] {
    override def filter(value: String): Boolean = {
      value.contains("flink")
    }
  }
)
----

我们filter的字符串"flink"还可以当作参数传进去。

[source,scala]
----
val tweets: DataStream[String] = ...
val flinkTweets = tweets.filter(new KeywordFilter("flink"))

class KeywordFilter(keyWord: String) extends FilterFunction[String] {
  override def filter(value: String): Boolean = {
    value.contains(keyWord)
  }
}
----

==== 4.8.2 Lambda Functions

[source,scala]
----
val tweets: DataStream[String] = ...
val flinkTweets = tweets.filter(_.contains("flink"))
----

==== 4.8.3 Rich Functions

* RichMapFunction
* RichFlatMapFunction
* RichFilterFunction
* ...

所有Flink提供的Function都有Rich版本，提供了更丰富的功能。

Rich Function有一个生命周期的概念

* open()方法是rich function的初始化方法，当一个算子例如map或者filter被调用之前open()会被调用。
* close()方法是生命周期中的最后一个调用的方法，做一些清理工作。
* getRuntimeContext()方法提供了函数的RuntimeContext的一些信息，例如函数执行的并行度，任务的名字，以及state状态

[source,scala]
----
class MyFlatMap extends RichFlatMapFunction[Int, (Int, Int)] {
  var subTaskIndex = 0

  override def open(configuration: Configuration): Unit = {
    subTaskIndex = getRuntimeContext.getIndexOfThisSubtask
    // 做一些初始化工作
    // 例如建立一个和HDFS的连接
  }

  override def flatMap(in: Int, out: Collector[(Int, Int)]): Unit = {
    if (in % 2 == subTaskIndex) {
      out.collect((subTaskIndex, in))
    }
  }

  override def close(): Unit = {
    // 清理工作，断开和HDFS的连接。
  }
}
----

=== 4.9 Sink

Flink没有类似于Spark中foreach方法，让用户进行迭代的操作。所有对外的输出操作都要利用Sink完成。最后通过类似如下方式完成整个任务最终输出操作。

[source,scala]
----
stream.addSink(new MySink(xxxx)) 
----

官方提供了一部分的框架的sink。除此以外，需要用户自定义实现sink。   

==== 4.9.1 Elasticsearch

[source,xml]
----
<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch6_${scala.binary.version}</artifactId>
    <version>${flink.version}</version>
</dependency>
----

[source,scala]
----
import org.apache.flink.api.common.functions.RuntimeContext
import org.apache.flink.streaming.api.datastream.DataStream
import org.apache.flink.streaming.connectors.elasticsearch.ElasticsearchSinkFunction
import org.apache.flink.streaming.connectors.elasticsearch.RequestIndexer
import org.apache.flink.streaming.connectors.elasticsearch6.ElasticsearchSink

import org.apache.http.HttpHost
import org.elasticsearch.action.index.IndexRequest
import org.elasticsearch.client.Requests

import java.util.ArrayList
import java.util.List

val input: DataStream[String] = ...

val httpHosts = new java.util.ArrayList[HttpHost]
httpHosts.add(new HttpHost("127.0.0.1", 9200, "http"))

val esSinkBuilder = new ElasticsearchSink.Builer[String](
  httpHosts,
  new ElasticsearchSinkFunction[String] {
    def createIndexRequest(element: String): IndexRequest = {
      val json = new java.util.HashMap[String, String]
      json.put("data", element)
      
      return Requests.indexRequest()
              .index("my-index")
              .type("my-type")
              .source(json)
    }

    override def process(element: String, ctx: RuntimeContext, indexer: RequestIndexer) {
      indexer.add(createIndexRequest(element))
    }
  }
)

// finally, build and add the sink to the job's pipeline
input.addSink(esSinkBuilder.build)
----